{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, recall_score, accuracy_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FEATURES = ['category_encoded',\n",
    "                  'amt', \n",
    "                  'gender_encoded', \n",
    "                  'city_encoded', \n",
    "                  'state_encoded', \n",
    "                  'city_pop', \n",
    "                  'job_encoded', \n",
    "                  'age', \n",
    "                  'hour', \n",
    "                  'daily', \n",
    "                  'day', \n",
    "                  'month']\n",
    "OUTPUT_FEATURE = ['is_fraud']\n",
    "\n",
    "def preprocessing(df):\n",
    "    # remove duplicates\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    # drop nA rows containing values\n",
    "    df.dropna(axis=0)\n",
    "    # drop the 'Unnamed: 0'\n",
    "    df = df.drop('Unnamed: 0', axis=1)\n",
    "    # change the type of date time\n",
    "    df['age'] = df['dob'].apply(lambda x: datetime.now().year - datetime.strptime(x, '%Y-%m-%d').year)\n",
    "    df['trans_datetime'] = pd.to_datetime(df['trans_date_trans_time'])\n",
    "    df['hour'] = df['trans_datetime'].dt.hour\n",
    "    df['daily'] = df['trans_datetime'].dt.day\n",
    "    df['day'] = df['trans_datetime'].dt.dayofweek\n",
    "    df['month'] = df['trans_datetime'].dt.month\n",
    "    df.drop('trans_date_trans_time', axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def smoteTomek_augmentation(df, sampling_strategy):\n",
    "    # define the model\n",
    "    smote_tomek = SMOTETomek(random_state=42, sampling_strategy=sampling_strategy)\n",
    "\n",
    "    X_Augmented, y_augmented = smote_tomek.fit_resample(df[INPUT_FEATURES],df[OUTPUT_FEATURE])\n",
    "\n",
    "    new_df = pd.concat([X_Augmented, y_augmented], axis=1)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def encoding_columns(df):\n",
    "\n",
    "    labelencoder = LabelEncoder()\n",
    "    df['category_encoded'] = labelencoder.fit_transform(df['category'])\n",
    "    df['gender_encoded'] = labelencoder.fit_transform(df['gender'])\n",
    "    df['city_encoded'] = labelencoder.fit_transform(df['city'])\n",
    "    df['state_encoded'] =labelencoder.fit_transform(df['state'])\n",
    "    df['job_encoded'] = labelencoder.fit_transform(df['job'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def dataloading(for_training, input_features, output_feature, augmented=False, sampling_strategy=0.3):\n",
    "\n",
    "    if for_training:\n",
    "        df = pd.read_csv('../data/fraudTrain.csv')\n",
    "        df = df[df['is_fraud']==0]\n",
    "    else:\n",
    "        df = pd.read_csv('../data/fraudTest.csv')\n",
    "    \n",
    "    df = preprocessing(df)\n",
    "    df = encoding_columns(df)\n",
    "\n",
    "        # Augment the data\n",
    "    if augmented == True:\n",
    "        df = smoteTomek_augmentation(df, sampling_strategy)\n",
    "\n",
    "\n",
    "    if for_training:\n",
    "        # split btwn training data and validation with ratio 90%\n",
    "        df_train, df_val = train_test_split(df, test_size=0.1, random_state=42, stratify=df['is_fraud'])\n",
    "\n",
    "        # scale the data\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(df_train[input_features])\n",
    "\n",
    "        df_train[input_features]=scaler.transform(df_train[input_features])\n",
    "        df_val[input_features]=scaler.transform(df_val[input_features])\n",
    "\n",
    "        # separate Input and Label\n",
    "        X_train = df_train[input_features]\n",
    "        y_train = df_train[output_feature]\n",
    "\n",
    "        X_val = df_val[input_features]\n",
    "        y_val = df_val[output_feature]  \n",
    "\n",
    "\n",
    "        return X_train, y_train,X_val, y_val\n",
    "\n",
    "    else:\n",
    "\n",
    "        # scale the data\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(df[input_features])\n",
    "        df[input_features]=scaler.transform(df[input_features])\n",
    "\n",
    "        X_test = df[input_features]\n",
    "        y_test = df[output_feature]  \n",
    "\n",
    "        return X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val = dataloading(True, INPUT_FEATURES, OUTPUT_FEATURE)\n",
    "X_test, y_test = dataloading(False, INPUT_FEATURES, OUTPUT_FEATURE, augmented=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anomaly_scores(df_original, df_restored):\n",
    "    loss = np.sum((np.array(df_original) - np.array(df_restored)) ** 2, axis=1)\n",
    "    loss = pd.Series(data=loss, index=df_original.index)\n",
    "    return loss\n",
    "\n",
    "def is_anomaly(data, pca, threshold):\n",
    "    pca_data = pca.transform(data)\n",
    "    restored_data = pca.inverse_transform(pca_data)\n",
    "    loss = np.sum((data - restored_data) ** 2)\n",
    "    return loss > threshold\n",
    "\n",
    "\n",
    "# fit to the i principal components\n",
    "pca = PCA(n_components=8, random_state=0)\n",
    "pca.fit_transform(X_train)\n",
    "\n",
    "df_pca = pd.DataFrame(pca.transform(X_test), index=X_test.index)\n",
    "df_restored = pd.DataFrame(pca.inverse_transform(df_pca), index=df_pca.index)\n",
    "\n",
    "reconstruction_errors = get_anomaly_scores(X_test, df_restored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(nm_PCA, X_train, X_val):\n",
    "    pca = PCA(n_components=nm_PCA, random_state=0)\n",
    "    pca.fit_transform(X_train)\n",
    "\n",
    "    df_pca = pd.DataFrame(pca.transform(X_val), index=X_val.index)\n",
    "    df_restored = pd.DataFrame(pca.inverse_transform(df_pca), index=df_pca.index)\n",
    "\n",
    "    reconstruction_errors = get_anomaly_scores(X_val, df_restored)\n",
    "\n",
    "    theta = np.mean(reconstruction_errors)\n",
    "\n",
    "    return pca, theta\n",
    "\n",
    "def test(pca, X_test, y_test, threshold):\n",
    "    \n",
    "    df_pca = pd.DataFrame(pca.transform(X_test), index=X_test.index)\n",
    "    df_restored = pd.DataFrame(pca.inverse_transform(df_pca), index=df_pca.index)\n",
    "    \n",
    "    reconstruction_errors = get_anomaly_scores(X_test, df_restored)\n",
    "\n",
    "    pred = (reconstruction_errors > threshold).astype(int)\n",
    "    error_df = pd.DataFrame({'pred': pred.to_numpy(),\n",
    "                           'True_class': y_test['is_fraud'].to_numpy()})\n",
    "    \n",
    "    return error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca, theta = train(4, X_train, X_val)\n",
    "error_df = test(pca, X_test, y_test, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = []\n",
    "for i in range(2, 9):\n",
    "    \n",
    "    pca, theta = train(i, X_train, X_val)\n",
    "    error_df = test(pca, X_test, y_test, theta)\n",
    "    values = [i,accuracy_score(error_df['True_class'], error_df['pred']),recall_score(error_df['True_class'], error_df['pred']),precision_score(error_df['True_class'], error_df['pred']),f1_score(error_df['True_class'], error_df['pred'])]\n",
    "    report.append(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy:  0.7149386650447438\n",
      " Recall:  0.6983682983682984\n",
      " Precision:  0.009405707468684268\n",
      " F1_score:  0.018561427420853725\n"
     ]
    }
   ],
   "source": [
    "print(\" Accuracy: \",accuracy_score(error_df['True_class'], error_df['pred']))\n",
    "print(\" Recall: \",recall_score(error_df['True_class'], error_df['pred']))\n",
    "print(\" Precision: \",precision_score(error_df['True_class'], error_df['pred']))\n",
    "print(\" F1_score: \",f1_score(error_df['True_class'], error_df['pred']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp0197-pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
